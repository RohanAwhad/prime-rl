# Cascade LLM Architecture Implementation Plan

## Goal
Implement a two-stage LLM architecture: `Query → M1 (frozen) → M2 (trainable) → Answer`

## Architecture Overview
```
Query → M1 (frozen, port 8001) → draft → M2 (trainable, port 8000) → refined answer → reward
```

- **M1**: Frozen drafter - generates initial answer
- **M2**: Trainable refiner - reviews and corrects M1's draft
- Only M2 receives weight updates during RL training

## Implementation Strategy

### Approach: Custom Verifiers Environment Package

Create a Python package `cascade_refine` with a `load_environment()` function that:
1. Wraps a base environment (e.g., gsm8k)
2. Before each rollout, calls M1 to generate a draft
3. Modifies the prompt to include the draft for M2
4. Delegates scoring to the base environment

### Key Insight from Codebase Exploration

**Environment loading** (`verifiers/utils/env_utils.py:9-75`):
```python
module_name = env_id.replace("-", "_").split("/")[-1]
module = importlib.import_module(module_name)
env_instance = module.load_environment(**env_args)
```

So we need a package named `cascade_refine` with a `load_environment()` function.

**M1 stays frozen** because:
- `client.base_url` list only contains M2's URL
- `update_weights()` (`utils/client.py:104-126`) only updates clients in that list
- M1 is accessed by the environment directly, not via orchestrator's clients

---

## Files to Create

### 1. `src/prime_rl/envs/cascade_refine/__init__.py`
- Entry point for the environment package
- Exports `load_environment()` function

### 2. `src/prime_rl/envs/cascade_refine/env.py`
- `CascadeRefineEnv` class extending `vf.MultiTurnEnv`
- Override `run_group()` to inject M1 calls
- Store M1 client and configuration

### 3. `configs/cascade/m1_infer.toml`
- M1 inference server config (port 8001, no weight updates)

### 4. `configs/cascade/m2_infer.toml`
- M2 inference server config (port 8000, standard config)

### 5. `configs/cascade/orch.toml`
- Orchestrator config pointing to M2 only
- Uses `cascade_refine` environment with M1 config in args

### 6. `configs/cascade/train.toml`
- Standard RL trainer config for M2

### 7. `configs/cascade/rl.toml`
- Combined config for `uv run rl` (references train/orch/infer)

### 8. `scripts/run_cascade.sh`
- Orchestration script that starts M1, waits for health, runs RL, handles cleanup

---

## Detailed Implementation

### Step 1: Create `CascadeRefineEnv` class

```python
# src/prime_rl/envs/cascade_refine/env.py

class CascadeRefineEnv(vf.MultiTurnEnv):
    def __init__(
        self,
        base_env: vf.Environment,
        m1_base_url: str,
        m1_model: str,
        m1_sampling_args: dict | None = None,
        refine_template: str = "...",
        **kwargs,
    ):
        # Don't call super().__init__() with dataset - we'll delegate to base_env
        self.base_env = base_env
        self.m1_client = AsyncOpenAI(base_url=m1_base_url, api_key="EMPTY")
        self.m1_model = m1_model
        self.m1_sampling_args = m1_sampling_args or {}
        self.refine_template = refine_template

        # Copy necessary attributes from base_env
        self.dataset = base_env.dataset
        self.eval_dataset = base_env.eval_dataset
        self.rubric = base_env.rubric
        self.parser = base_env.parser
        # ... etc

    async def run_group(self, group_inputs, client, model, **kwargs):
        # 1. Get M1 drafts for all inputs
        drafts = await self._get_m1_drafts([inp["prompt"] for inp in group_inputs])

        # 2. Modify inputs to include drafts
        modified_inputs = self._inject_drafts(group_inputs, drafts)

        # 3. Delegate to base_env's run_group
        return await self.base_env.run_group(modified_inputs, client, model, **kwargs)
```

### Step 2: Create `load_environment()` function

```python
# src/prime_rl/envs/cascade_refine/__init__.py

def load_environment(
    base_env_id: str,
    m1_base_url: str,
    m1_model: str,
    m1_sampling_args: dict | None = None,
    refine_template: str | None = None,
    **base_env_args,
) -> CascadeRefineEnv:
    base_env = vf.load_environment(base_env_id, **base_env_args)
    return CascadeRefineEnv(
        base_env=base_env,
        m1_base_url=m1_base_url,
        m1_model=m1_model,
        m1_sampling_args=m1_sampling_args,
        refine_template=refine_template or DEFAULT_TEMPLATE,
    )
```

### Step 3: Make package discoverable

Add to `pyproject.toml`:
```toml
[tool.setuptools.packages.find]
where = ["src"]
include = ["prime_rl*"]
```

Or ensure `src/prime_rl/envs/cascade_refine` is importable as `cascade_refine`.

**Alternative**: Register as entry point or add to Python path.

### Step 4: Create config files

**`configs/cascade/orch.toml`**:
```toml
[client]
base_url = ["http://localhost:8000/v1"]  # Only M2

[model]
name = "Qwen/Qwen3-0.6B"

[[env]]
id = "cascade_refine"
args = {
    base_env_id = "reverse-text",
    m1_base_url = "http://localhost:8001/v1",
    m1_model = "Qwen/Qwen3-0.6B"
}
```

---

## Testing Plan

1. **Unit test**: Test `CascadeRefineEnv` with mock M1 client
2. **Integration test**: Run with two local vLLM servers
3. **Verify M1 frozen**: Check no weight updates go to port 8001
4. **End-to-end**: Full RL training loop on simple task (reverse-text)

---

## Decisions

- **Package location**: Inside prime-rl at `src/prime_rl/envs/cascade_refine/`
- **Test environment**: `reverse-text` (simplest, fast iteration)
- **Model setup**: Same model for M1/M2 (Qwen3-0.6B)

---

## Execution Order

1. Create `src/prime_rl/envs/cascade_refine/__init__.py` with `load_environment()`
2. Create `src/prime_rl/envs/cascade_refine/env.py` with `CascadeRefineEnv`
3. Create config files in `configs/cascade/`
4. Create `scripts/run_cascade.sh` - orchestration script
5. Test with `reverse-text` environment locally

---

## Script: `scripts/run_cascade.sh`

Based on existing `scripts/run_workload.sh` pattern, the cascade script must:

### Responsibilities
1. **Start M1 inference server** (frozen drafter) on port 8001
2. **Wait for M1 health check** before proceeding
3. **Run `uv run rl`** with cascade configs (spawns M2 + trainer + orchestrator)
4. **Handle signals** (Ctrl+C / SIGTERM) to clean up all processes
5. **Exit cleanly** when RL training completes or is interrupted

### Script Structure
```bash
#!/bin/bash
# Cascade LLM Training: M1 (frozen) → M2 (trainable)

# Cleanup function
cleanup() {
    echo "Stopping all processes..."
    kill $M1_PID 2>/dev/null
    wait
    exit 0
}
trap cleanup SIGINT SIGTERM

# Start M1 (frozen drafter) - runs in background
echo "[*] Starting M1 inference server (frozen) on port 8001..."
uv run inference @ configs/cascade/m1_infer.toml &
M1_PID=$!

# Wait for M1 to be healthy
echo "[*] Waiting for M1 server..."
for i in {1..60}; do
    if curl -s http://localhost:8001/health > /dev/null 2>&1; then
        echo "[✓] M1 server ready"
        break
    fi
    sleep 2
done

# Run RL training (spawns M2 + trainer + orchestrator)
echo "[*] Starting cascade RL training..."
uv run rl @ configs/cascade/rl.toml

# Cleanup on exit
cleanup
```

### Config File: `configs/cascade/rl.toml`
Combined config for `uv run rl` that specifies:
- `--trainer @ configs/cascade/train.toml`
- `--orchestrator @ configs/cascade/orch.toml`
- `--inference @ configs/cascade/m2_infer.toml`

Or inline as single TOML with sections.
